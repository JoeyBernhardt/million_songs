---
title: "dplyr functions with the singer dataset"
output:
  html_document:
    toc: true
    toc_depth: 4
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

### Where were we?

In the [introduction to dplyr](block009_dplyr-intro.html), we used two very important verbs and an operator:

  * `filter()` for subsetting data with row logic
  * `select()` for subsetting data variable- or column-wise
  * the pipe operator `%>%`, which feeds the LHS as the first argument to the expression on the RHS
  
We also discussed dplyr's role inside the tidyverse and tibbles:

  * dplyr is a core package in the [tidyverse](https://github.com/hadley/tidyverse) meta-package. Since we often make incidental usage of the others, we will load dplyr and the others via `library(tidyverse)`.
  * The tidyverse embraces a special flavor of data frame, called a tibble. The `gapminder` dataset is stored as a tibble.  

### Load dplyr and gapminder

I choose to load the tidyverse, which will load dplyr, among other packages we use incidentally below.

```{r}
library(tidyverse)
library(gapminder)
```

### Read in the songs data

We're going to make changes to the `gapminder` tibble. To eliminate any fear that you're damaging the data that comes with the package, we create an explicit copy of `gapminder` for our experiments.

```{r}
# (my_gap <- gapminder)
singer_locations <- read_csv("data-processed/singer_locations.csv")
```

**Pay close attention** to when we evaluate statements but let the output just print to screen:

```{r eval = FALSE}
## let output print to screen, but do not store
singer_locations %>% filter(city == "New York")
```

... versus when we assign the output to an object, possibly overwriting an existing object.

```{r eval = FALSE}
## store the output as an R object
new_york <- singer_locations %>% filter(city == "New York")
```

We can use `filter` to remove rows with empty data

```{r}
singer_locations2 <- singer_locations %>% 
  filter(!is.na(artist_hotttnesss), !is.na(artist_familiarity), !is.na(artist_name), !is.na(city)) %>% 
  filter(artist_hotttnesss > 0, artist_familiarity > 0) 
```


### Use `mutate()` to add new variables

Imagine we wanted to create a new variable that represents artist popularity -- it will combine information about artist familiarity and artist hotness.

`mutate()` is a function that defines and inserts new variables into a tibble. You can refer to existing variables by name.

```{r}
my_songs <- singer_locations2 %>%
   mutate(artist_popularity = artist_hotttnesss*artist_familiarity) 
head(my_songs)
```

Those numbers might seem abstract -- what does a popularity score of 0.41 mean? What if we scale our artist popularity scores relative to the most popular artist in the dataset, Rihanna. 

I need to create a new variable that is `artist_popularity` divided by Rihanna's `artist_popularity`.

We can do this by:

  * Filter down to the rows for Rihanna.
  * Pull out the popularity value for Rihanna, and add that value to a new column (`rihanna_popularity`)
  * Divide raw `artist_popularity` by Rihanna's popularity (`rihanna_popularity`)
  
```{r}
rihanna <- my_songs %>% 
  filter(artist_name == "Rihanna", title == "Disturbia")

my_songs2 <- my_songs %>%
  mutate(rihanna_popularity = rihanna$artist_popularity,
         relative_popularity = artist_popularity / rihanna_popularity)
```

Note that, `mutate()` builds new variables sequentially so you can reference earlier ones (like `rihanna_popularity`) when defining later ones (like `relative_popularity`). Also, you can get rid of a variable by setting it to `NULL`.

How could we sanity check that this worked? The values in the Rihanna rows for `relative_popularity` better all be 1!

```{r}
### sanity check, all the rows here should be 1
my_songs2 %>% 
  filter(artist_name == "Rihanna") %>% 
  select(artist_name, title,  relative_popularity)
```

Check your intuition for how all the artists in the dataset should stack up against Rihanna.

```{r}
summary(my_songs2$relative_popularity)
```


Remember: Trust No One. Including (especially?) yourself. Always try to find a way to check that you've done what meant to. Prepare to be horrified.

### Use `arrange()` to row-order data in a principled way

`arrange()` reorders the rows in a data frame. Imagine you wanted this data ordered by artist name then year. 

```{r}
my_songs %>%
  arrange(artist_name, year)
```

Or maybe you want just the data from 2006, sorted on artist familiarity?

```{r}
my_songs %>%
  filter(year == 2006) %>%
  arrange(artist_familiarity)
```

Oh, you'd like to sort on artist familiarity in **desc**ending order? Then use `desc()`.

```{r}
my_songs %>%
  filter(year == 2006) %>%
  arrange(desc(artist_familiarity))
```

I advise that your analyses NEVER rely on rows or variables being in a specific order. But it's still true that human beings write the code and the interactive development process can be much nicer if you reorder the rows of your data as you go along. Also, once you are preparing tables for human eyeballs, it is imperative that you step up and take control of row order.

### Use `rename()` to rename variables

Let's say the spelling of `artist_hotttnesss` gives you the creeps and you want to change it, and you want rename the `city` column to be called `location`.

```{r}
my_songs %>%
  rename(artist_hotness = artist_hotttnesss,
         location = city)
```

I did NOT assign the post-rename object back to `my_songs` because that would make the chunks in this tutorial harder to copy/paste and run out of order. In real life, I would probably assign this back to `my_songs`, in a data preparation script, and proceed with the new variable names.

### `select()` can rename and reposition variables

You've seen simple use of `select()`. There are two tricks you might enjoy:

  1. `select()` can rename the variables you request to keep.
  1. `select()` can be used with `everything()` to hoist a variable up to the front of the tibble.
  
```{r}
my_songs %>%
  filter(artist_name == "Hot Chip", year > 1996) %>% 
  select(yr = year, artist_familiarity, hotness = artist_hotttnesss) %>% 
  select(artist_familiarity, everything())
```

`everything()` is one of several helpers for variable selection. Read its help to see the rest.

### `group_by()` is a mighty weapon

I have found ~~friends and family~~ collaborators love to ask seemingly innocuous questions like, "which artist has wideest range of song durations?". In fact, that is a totally natural question to ask. But if you are using a language that doesn't know about data, it's an incredibly annoying question to answer.

dplyr offers powerful tools to solve this class of problem.

  * `group_by()` adds extra structure to your dataset -- grouping information -- which lays the groundwork for computations within the groups.
  * `summarize()` takes a dataset with $n$ observations, computes requested summaries, and returns a dataset with 1 observation.
  * Window functions take a dataset with $n$ observations and return a dataset with $n$ observations.
  * `mutate()` and `summarize()` will honor groups.
  * You can also do very general computations on your groups with `do()`, though elsewhere in this course, I advocate for other approaches that I find more intuitive, using the `purrr` package.
  
Combined with the verbs you already know, these new tools allow you to solve an extremely diverse set of problems with relative ease.

#### Counting things up

Let's start with simple counting.  How many observations do we have per artist?

```{r}
my_songs %>%
  group_by(artist_name) %>%
  summarize(n = n())
```

Let us pause here to think about the tidyverse. You could get these same frequencies using `table()` from base R.

```{r}
table(my_songs$artist_name)
str(table(my_songs$artist_name))
```

But the object of class `table` that is returned makes downstream computation a bit fiddlier than you'd like. For example, it's too bad the continent levels come back only as *names* and not as a proper factor, with the original set of levels. This is an example of how the tidyverse smooths transitions where you want the output of step i to become the input of step i + 1.

The `tally()` function is a convenience function that knows to count rows. It honors groups.

```{r}
my_songs %>%
  group_by(artist_name) %>%
  tally()
```

The `count()` function is an even more convenient function that does both grouping and counting.

```{r}
my_songs %>% 
  count(artist_name)
```

What if we wanted to add the number of unique tracks for each artist? You can compute multiple summaries inside `summarize()`. Use the `n_distinct()` function to count the number of distinct tracks for each artist.

```{r}
my_songs %>%
  group_by(artist_name) %>%
  summarize(n = n(),
            n_tracks = n_distinct(track_id))
```

#### General summarization

The functions you'll apply within `summarize()` include classical statistical summaries, like  `mean()`, `median()`, `var()`, `sd()`, `mad()`, `IQR()`, `min()`, and `max()`. Remember they are functions that take $n$ inputs and distill them down into 1 output.

Let's compute the average song duration per year, to see if there are any trends in song duration over time.

```{r}
my_songs %>%
  group_by(year) %>%
  summarize(avg_duration = mean(duration))
```

`summarize_each()` applies the same summary function(s) to multiple variables. Let's compute average and median song duration and hotness by year ... but only for 1952 and 2007.

```{r}
my_songs %>%
  filter(year %in% c(1952, 2007)) %>%
  group_by(year) %>%
  summarise_each(funs(mean, median), duration, artist_hotttnesss)
```

Let's focus just on the artist named Hot Tuna. What are the minimum and maximum song durations seen by year?

```{r}
my_songs %>%
  filter(artist_name == "Hot Tuna") %>%
  group_by(year) %>%
  summarize(min_duration = min(duration), max_duration = max(duration))
```

*STILL working on this section, it needs more purposeful song sampling to be interesting.*

Of course it would be much more interesting to see *which* country contributed these extreme observations. Is the minimum (maximum) always coming from the same country? We tackle that with window functions shortly.

### Grouped mutate

Sometimes you don't want to collapse the $n$ rows for each group into one row. You want to keep your groups, but compute within them.

#### Computing with group-wise summaries

*placeholder text*
Let's make a new variable that is the years of life expectancy gained (lost) relative to 1952, for each individual country. We group by country and use `mutate()` to make a new variable. The `first()` function extracts the first value from a vector. Notice that `first()` is operating on the vector of life expectancies *within each country group*.

```{r}
my_songs %>% 
  group_by(artist_name) %>% 
  select(year, artist_name, duration, artist_familiarity) %>% 
  mutate(durating_gain = duration - first(duration)) %>% 
  filter(year > 1963)
```

Within country, we take the difference between life expectancy in year $i$ and life expectancy in 1952. Therefore we always see zeroes for 1952 and, for most countries, a sequence of positive and increasing numbers.

#### Window functions

Window functions take $n$ inputs and give back $n$ outputs. Furthermore, the output depends on all the values. So `rank()` is a window function but `log()` is not. Here we use window functions based on ranks and offsets.

Let's revisit the worst and best life expectancies in Asia over time, but retaining info about *which* country contributes these extreme values.

```{r}
my_songs %>%
  filter(artist_name == "Hot Tuna") %>%
  select(year, duration, artist_familiarity) %>%
  group_by(year) %>%
  filter(min_rank(desc(duration)) < 2 | min_rank(duration) < 2) %>% 
  arrange(year) %>%
  print(n = Inf)
```

We see that (min = Afghanistan, max = Japan) is the most frequent result, but Cambodia and Israel pop up at least once each as the min or max, respectively. That table should make you impatient for our upcoming work on tidying and reshaping data! Wouldn't it be nice to have one row per year?

How did that actually work? First, I store and view a partial that leaves off the `filter()` statement. All of these operations should be familiar.

```{r}
asia <- my_gap %>%
  filter(continent == "Asia") %>%
  select(year, country, lifeExp) %>%
  group_by(year)
asia
```

Now we apply a window function -- `min_rank()`. Since `asia` is grouped by year, `min_rank()` operates within mini-datasets, each for a specific year. Applied to the variable `lifeExp`, `min_rank()` returns the rank of each country's observed life expectancy. FYI, the `min` part just specifies how ties are broken. Here is an explicit peek at these within-year life expectancy ranks, in both the (default) ascending and descending order.

For concreteness, I use `mutate()` to actually create these variables, even though I dropped this in the solution above. Let's look at a bit of that.

```{r}
asia %>%
  mutate(le_rank = min_rank(lifeExp),
         le_desc_rank = min_rank(desc(lifeExp))) %>% 
  filter(country %in% c("Afghanistan", "Japan", "Thailand"), year > 1995)
```

Afghanistan tends to present 1's in the `le_rank` variable, Japan tends to present 1's in the `le_desc_rank` variable and other countries, like Thailand, present less extreme ranks.

You can understand the original `filter()` statement now:

```{r eval = FALSE}
filter(min_rank(desc(lifeExp)) < 2 | min_rank(lifeExp) < 2)
```

These two sets of ranks are formed on-the-fly, within year group, and `filter()` retains rows with rank less than 2, which means ... the row with rank = 1. Since we do for ascending and descending ranks, we get both the min and the max.

If we had wanted just the min OR the max, an alternative approach using `top_n()` would have worked.

```{r}
my_gap %>%
  filter(continent == "Asia") %>%
  select(year, country, lifeExp) %>%
  arrange(year) %>%
  group_by(year) %>%
  #top_n(1, wt = lifeExp)        ## gets the min
  top_n(1, wt = desc(lifeExp)) ## gets the max
```

### Grand Finale

So let's answer that "simple" question: which country experienced the sharpest 5-year drop in life expectancy? Recall that this excerpt of the Gapminder data only has data every five years, e.g. for 1952, 1957, etc. So this really means looking at life expectancy changes between adjacent timepoints.

At this point, that's just too easy, so let's do it by continent while we're at it.

```{r}
my_gap %>%
  select(country, year, continent, lifeExp) %>%
  group_by(continent, country) %>%
  ## within country, take (lifeExp in year i) - (lifeExp in year i - 1)
  ## positive means lifeExp went up, negative means it went down
  mutate(le_delta = lifeExp - lag(lifeExp)) %>% 
  ## within country, retain the worst lifeExp change = smallest or most negative
  summarize(worst_le_delta = min(le_delta, na.rm = TRUE)) %>% 
  ## within continent, retain the row with the lowest worst_le_delta
  top_n(-1, wt = worst_le_delta) %>% 
  arrange(worst_le_delta)
```

Ponder that for a while. The subject matter and the code. Mostly you're seeing what genocide looks like in dry statistics on average life expectancy.

Break the code into pieces, starting at the top, and inspect the intermediate results. That's certainly how I was able to *write* such a thing. These commands do not [leap fully formed out of anyone's forehead](http://tinyurl.com/athenaforehead) -- they are built up gradually, with lots of errors and refinements along the way. I'm not even sure it's a great idea to do so much manipulation in one fell swoop. Is the statement above really hard for you to read? If yes, then by all means break it into pieces and make some intermediate objects. Your code should be easy to write and read when you're done.

In later tutorials, we'll explore more of dplyr, such as operations based on two datasets.

### Resources

`dplyr` official stuff

  * package home [on CRAN](http://cran.r-project.org/web/packages/dplyr/index.html)
    - note there are several vignettes, with the [introduction](http://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html) being the most relevant right now
    - the [one on window functions](http://cran.rstudio.com/web/packages/dplyr/vignettes/window-functions.html) will also be interesting to you now
  * development home [on GitHub](https://github.com/hadley/dplyr)
  * [tutorial HW delivered](https://www.dropbox.com/sh/i8qnluwmuieicxc/AAAgt9tIKoIm7WZKIyK25lh6a) (note this links to a DropBox folder) at useR! 2014 conference

[RStudio Data Wrangling cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf), covering `dplyr` and `tidyr`. Remember you can get to these via *Help > Cheatsheets.* 

[Data transformation](http://r4ds.had.co.nz/transform.html) chapter of [R for Data Science](http://r4ds.had.co.nz)

[Excellent slides](https://github.com/tjmahr/MadR_Pipelines) on pipelines and `dplyr` by TJ Mahr, talk given to the Madison R Users Group.

Blog post [Hands-on dplyr tutorial for faster data manipulation in R](http://www.dataschool.io/dplyr-tutorial-for-faster-data-manipulation-in-r/) by Data School, that includes a link to an R Markdown document and links to videos

[Cheatsheet](bit001_dplyr-cheatsheet.html) I made for `dplyr` join functions (not relevant yet but soon)